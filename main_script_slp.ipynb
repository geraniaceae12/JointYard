{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "import rmm\n",
    "import cupy as cp\n",
    "\n",
    "from datetime import datetime\n",
    "from preprocess.loadConfig import ConfigLoader\n",
    "from preprocess.loadLPdata import LoadLPData\n",
    "from preprocess.reconstruct3D import process_groups_reconstruction\n",
    "from preprocess.egocentric import egocentric_alignment_3d, align_3dvector_to_axis\n",
    "from preprocess.computemotion import compute_vector_magnitudes, compute_velocities,compute_direction_changes\n",
    "from preprocess.addposition import calculate_position\n",
    "from preprocess.addangle import calculate_normal_vectors_per_group, calculate_angle_between_line_and_plane\n",
    "from analysis.dim_reduction.pca import perform_pca, save_pca_results\n",
    "from analysis.cwt import CWT, cwt_filter\n",
    "from analysis.dim_reduction.umap import perform_umap, plot_umap\n",
    "from analysis.embedding.vae.vae_main import vae_run\n",
    "from analysis.embedding.vae.vae_visualize import extract_latent_space\n",
    "from analysis.embedding.vae.vae_utils import numpy_to_tensor, load_savedmodel, load_hyperparameters\n",
    "from analysis.segmentation.hdbscan.hdbscan_clustering import apply_clustering\n",
    "from analysis.segmentation.hdbscan.hdbscan_confirm import load_video_frames, plot_frames, extract_cluster_frame_indices\n",
    "from analysis.segmentation.hdbscan.hdbscan_medoid import calculate_cluster_medoids, plot_latent_space_with_medoids,plot_latent_space_with_medoids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Pose tracking dataset of Sleap-Anipose #########\n",
    "h5_files = [\n",
    "\"sleap-anipose result file pathways(.h5), that 3d coordinates time series dataset\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  Configuration ######################################################\n",
    "print(\"\\n***Reading the configuration file***\")\n",
    "config_path = 'file pathway/to/ .. /config_slp_sample.json'\n",
    "cfg = ConfigLoader(config_path)\n",
    "\n",
    "# Load configuration in each process\n",
    "info_cfg = cfg.get_info()\n",
    "cwt_cfg = cfg.get_cwt()\n",
    "vae_cfg = cfg.get_vae()\n",
    "umap_cfg = cfg.get_umap()\n",
    "hdbscan_cfg = cfg.get_hdbscan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create the index_info.csv file #########\n",
    "rows_per_file = #the number of frames in each video\n",
    "sourcefile_column = [file for file in h5_files for _ in range(rows_per_file)]\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame({\"Sourcefile\": sourcefile_column})\n",
    "\n",
    "# save into csv file\n",
    "df.to_csv(\"pathway/../index_info.csv\", index=False)\n",
    "\n",
    "print(\"Create the index_info.csv file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LOAD multiple h5 files\n",
    "all_data = []\n",
    "all_sources = []  # list of saving index \n",
    "\n",
    "for idx, file in enumerate(h5_files):\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        locations = f[\"tracks\"][:].squeeze()  # (n_frames, n_nodes, 3)\n",
    "        relocations = locations.reshape(locations.shape[0], -1)  # (n_frames, n_nodes*3)\n",
    "\n",
    "        output_dir = os.path.join(info_cfg[\"save_dir\"], \"npy\") \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        npy_filename = os.path.join(output_dir, os.path.basename(file).replace(\".h5\", \".npy\"))\n",
    "        np.save(npy_filename, relocations)\n",
    "\n",
    "        all_data.append(relocations)\n",
    "\n",
    "        source_idx = np.full((relocations.shape[0], 1), idx) \n",
    "        all_sources.append(source_idx)\n",
    "\n",
    "data = np.vstack(all_data)  # (total_frames, n_nodes*3)\n",
    "np.save(os.path.join(output_dir, \"total data.npy\"), data)\n",
    "np.save(os.path.join(output_dir, \"source.npy\"), np.vstack(all_sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Preprocess ########################################################\n",
    "#### Change the alignment: Allocentric to Egocentric\n",
    "# Egocentric Alignment\n",
    "ego_coords, center_coords = egocentric_alignment_3d(data, info_cfg[\"keypoint_names\"], \"body\")\n",
    "print(\"Convert 3Ddata into egocentric. Shape:\", ego_coords.shape)\n",
    "aligned_rotated_data = align_3dvector_to_axis(\n",
    "    ego_coords,\n",
    "    vector_from_name='body',  # same as egocentric_keypoint\n",
    "    vector_to_name='top_tail',\n",
    "    bodypoint_names=info_cfg[\"keypoint_names\"]\n",
    ")\n",
    "print(\"Egocentric-Axis Alignment completed for 3D data. Shape:\", aligned_rotated_data.shape)\n",
    "denoised_data = aligned_rotated_data\n",
    "\n",
    "#### Compute velocity: coords to velocities\n",
    "arr_velocity = compute_velocities(ego_coords)\n",
    "print(\"Completion of computing velocities. Shape:\", arr_velocity.shape)\n",
    "\n",
    "preprocess_path = os.path.join(info_cfg[\"save_dir\"],\"mouse_kaist_ego_axis_v.npy\")\n",
    "np.save(preprocess_path, arr_velocity)\n",
    "print(f\"Preprocess result saved to {preprocess_path}.\")\n",
    "\n",
    "denoised_data = arr_velocity\n",
    "\n",
    "# ### Denoising the input of CWT\n",
    "# _, _ , denoised_data,_, _ = perform_pca(arr_velocity, 0.95, using_gpu=False)\n",
    "# print(f\"After denoising, the cwt input data shape is :{denoised_data.shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### (Option) Concatenate posture and velocity BEFORE CWT ########\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_coords = StandardScaler()\n",
    "scaler_speed = StandardScaler()\n",
    "\n",
    "coords_scaled = scaler_coords.fit_transform(aligned_rotated_data)\n",
    "#coords_scaled = scaler_coords.fit_transform(ego_coords), choosing appropriate one that u want to use\n",
    "speed_scaled = scaler_speed.fit_transform(arr_velocity)\n",
    "\n",
    "denoised_data = np.concatenate([coords_scaled, speed_scaled], axis=1)\n",
    "print(f\"The CWT input data shape is : {denoised_data.shape}, concatenated posture and velocity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Continuous Wavelet Transform ##########################################\n",
    "if cwt_cfg[\"use_saved_cwt\"]:\n",
    "    print(f\"\\n***Loaded precomputed CWT data from {cwt_cfg['use_savedcwt_path']}***\")\n",
    "    amp_reshaped = np.load(cwt_cfg[\"use_savedcwt_path\"])\n",
    "    print(\"Loaded CWT data. Shape.T:\", amp_reshaped.T.shape)\n",
    "else: \n",
    "    print(\"\\n***Performing Continuous wavelet transform***\")\n",
    "    # Create an instance of the CWT class\n",
    "    cwt = CWT(cwt_cfg[\"frequencies\"], cwt_cfg[\"cwt_omega0\"], cwt_cfg[\"cwt_dt\"], cwt_cfg[\"scaler\"])  \n",
    "\n",
    "    # Perform the wavelet transform\n",
    "    amp, W = cwt.fast_wavelet_morlet_convolution(denoised_data)\n",
    "\n",
    "    # Save the cwt result \n",
    "    L, N, num_features = amp.shape # (50,17546,104)\n",
    "    amp_reshaped = np.reshape(amp, (L*num_features, N)) # (50*104, 17546)\n",
    "\n",
    "    cwt_dir = os.path.join(info_cfg[\"save_dir\"], 'cwt')\n",
    "    os.makedirs(cwt_dir, exist_ok=True)\n",
    "\n",
    "    cwt_path = os.path.join(cwt_dir,f'cwt_{cwt_cfg[\"frequencies_start\"]}_{cwt_cfg[\"frequencies_end\"]}_{cwt_cfg[\"frequencies_step\"]}_{cwt_cfg[\"scaler\"]}.npy')\n",
    "    np.save(cwt_path, amp_reshaped)\n",
    "    print(f\"CWT result saved to {cwt_path}. Shape.T:\", amp_reshaped.T.shape)\n",
    "\n",
    "    # Plot and save the CWT results\n",
    "    if cwt_cfg[\"cwt_plot_separate\"]:\n",
    "        cwt.plot_cwt_separate(amp, save_path=cwt_dir)\n",
    "        print(\"Saved CWT plots\")\n",
    "    amp_reshaped_before = amp_reshaped\n",
    "    # Filtering and save the CWT results\n",
    "    if cwt_cfg['cwt_filtering']:\n",
    "        amp_reshaped, retained_freq = cwt_filter(amp,cwt_cfg['freq_removal_threshold'],cwt_dir)\n",
    "        cwt_path = os.path.join(cwt_dir,f'cwt_{cwt_cfg[\"frequencies_start\"]}_{cwt_cfg[\"frequencies_end\"]}_{cwt_cfg[\"frequencies_step\"]}_{cwt_cfg[\"scaler\"]}_filter.npy')\n",
    "        cwt_path2 = os.path.join(cwt_dir,f'cwt_{cwt_cfg[\"frequencies_start\"]}_{cwt_cfg[\"frequencies_end\"]}_{cwt_cfg[\"frequencies_step\"]}_{cwt_cfg[\"scaler\"]}_filterinfo.npy')\n",
    "        np.save(cwt_path, amp_reshaped)\n",
    "        np.savez(cwt_path2, *retained_freq)\n",
    "        print(f\"Complete filtering the CWT data, before VAE. saved to {cwt_path}\")\n",
    "        print('Shape.T:', amp_reshaped.T.shape)\n",
    "\n",
    "# # Denoising before performing vae\n",
    "# print(\"\\n***Denoising the input of VAE\")\n",
    "# amp_reshaped,_,_,_,_ = perform_pca(amp_reshaped.T, confidence = preprocess_cfg[\"pca_confidence\"])\n",
    "# amp_reshaped = amp_reshaped.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### (Option) Concatenate posture and velocity AFTER CWT ######## (n_features, n_samples)\n",
    "cwt_posture = np.load('path/to/../cwt_posture.npy')\n",
    "cwt_velocity = np.load('path/to/../cwt_posture.npy')\n",
    "\n",
    "# Scaling seperately BEFORE VAE and Save the result\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_posture  = StandardScaler()\n",
    "scaler_velocity = StandardScaler()\n",
    "\n",
    "cwt_scaled_posture = scaler_posture.fit_transform(cwt_posture.T)\n",
    "cwt_scaled_velocity = scaler_velocity.fit_transform(cwt_velocity.T)\n",
    "print(\"Finish scaling seperately before VAE\")\n",
    "\n",
    "cwt_concat = np.concatenate([cwt_scaled_posture, cwt_scaled_velocity], axis=1) # (n_samples, n_features)\n",
    "print(f\"The VAE input data shape.T is : {cwt_concat.shape}, concatenated cwt results of posture and velocity.\")\n",
    "cwt_concat_dir = os.path.join(info_cfg[\"save_dir\"], 'cwt')\n",
    "os.makedirs(cwt_concat_dir, exist_ok=True)\n",
    "np.save(os.path.join(cwt_concat_dir,'cwt_scaled_p_v.npy'), cwt_concat.T) # save into (n_features, n_samples) format\n",
    "\n",
    "amp_reshaped = cwt_concat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### (Option; Restart from vae) Load CWT RESULT(cwt_concat: cwt_scaled_posture + cwt_scaled_velocity) ################\n",
    "amp_reshaped = np.load('/home/jiyoung/Desktop/lai_240524/Post-pose3D/test_kaist/kaist_data/debug14_p_v_1/cwt/cwt_scaled_p_v.npy')\n",
    "print(f\"The VAE input data shape.T is : {amp_reshaped.T.shape}, concatenated cwt results of posture and velocity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Embedding: Variational AutoEncoder ##################################\n",
    "if not umap_cfg[\"use_saved_umap\"]:   \n",
    "    if vae_cfg[\"use_saved_latentspace\"]:\n",
    "        print(f\"\\n***Loaded VAE latentspace data from {vae_cfg['use_savedlatent_path']}***\")\n",
    "        latent_space = np.load(vae_cfg['use_savedlatent_path'])\n",
    "        print(\"Loaded VAE latent space. Shape:\",latent_space.shape)\n",
    "    else:\n",
    "        if vae_cfg[\"use_saved_vae\"]:\n",
    "            print(\"\\n***Using already existed vae_model***\")\n",
    "            amp_reshaped = numpy_to_tensor(amp_reshaped) # (17546,50*104) = (n_samples*n_features)\n",
    "            vae_trained_model, device = load_savedmodel(vae_cfg[\"vaemodel_type\"],vae_cfg[\"use_saved_vaemodel_path\"],\n",
    "                                                        vae_cfg[\"use_saved_vaehyparam_path\"], amp_reshaped.shape[1])\n",
    "        else:\n",
    "            print(\"\\n***Start training of vae_model***\") \n",
    "            vae_trained_model, device = vae_run(config_path, amp_reshaped)\n",
    "            # vae_trained_model = vae_run(config_path) # vae_cfg[\"data_path\"]에 원하는 Data(.npy) 경로 넣으면 해당 데이터로 vae run\n",
    "            amp_reshaped = numpy_to_tensor(amp_reshaped)\n",
    "\n",
    "        # Extract latent space from VAE model and save it\n",
    "        latent_space = extract_latent_space(vae_trained_model, amp_reshaped, device, batch_size=random.choice(vae_cfg[\"batch_size_options\"])) #(n_samples*n_features)\n",
    "        np.save(os.path.join(info_cfg[\"save_dir\"],'vae_latentspace.npy'),latent_space)\n",
    "        del vae_trained_model, amp_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Dim Reduction: UMAP ################################################\n",
    "if umap_cfg[\"use_saved_umap\"]:\n",
    "    print(f\"\\n***Loaded precomputed UMAP data from {umap_cfg['use_savedumap_path']}***\")\n",
    "    umap_space = np.load(umap_cfg[\"use_savedumap_path\"])\n",
    "else: \n",
    "    umap_space, umap_save_dir = perform_umap(latent_space, umap_cfg[\"n_neighbors\"],\n",
    "                                    umap_cfg[\"min_dist\"],umap_cfg[\"n_components\"],\n",
    "                                    info_cfg[\"save_dir\"])\n",
    "    plot_umap(umap_space, umap_cfg['n_neighbors'], umap_cfg['min_dist'], umap_cfg['plot_2d'],\n",
    "            umap_cfg['plot_3d'], umap_cfg['save_plot'], umap_save_dir)\n",
    "    del latent_space\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### if we need deterministic results, use cpu version!!!\n",
    "# import umap as umap_cpu\n",
    "# umap_model = umap_cpu.UMAP(\n",
    "#             n_neighbors= umap_cfg[\"n_neighbors\"],\n",
    "#             min_dist=umap_cfg[\"min_dist\"],\n",
    "#             n_components=umap_cfg[\"n_components\"],\n",
    "#             random_state=42\n",
    "#         )\n",
    "# umap_space_cpu = umap_model.fit_transform(latent_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Clustering: HDBSCAN ################################################\n",
    "min_cluster_sizes = hdbscan_cfg[\"min_cluster_sizes\"]\n",
    "for min_cluster_size in min_cluster_sizes:\n",
    "    print(f\"\\n***Running HDBSCAN with min_cluster_size = {min_cluster_size}***\")\n",
    "\n",
    "    # HDBSCAN\n",
    "    try:\n",
    "        reduced_data, cluster_labels, soft_labels, max_probs, index_info_df = apply_clustering(\n",
    "             umap_space, hdbscan_cfg, min_cluster_size, info_cfg[\"save_dir\"])\n",
    "        cluster_save_dir= os.path.join(info_cfg[\"save_dir\"],'hdbscan/pdf')\n",
    "        os.makedirs(cluster_save_dir, exist_ok=True)\n",
    "        #np.save(os.path.join(cluster_save_dir,'reduced_data.npy'),reduced_data)\n",
    "        np.save(os.path.join(cluster_save_dir,f'soft_labels_{hdbscan_cfg[\"method\"]}_{min_cluster_size}.npy'),soft_labels)\n",
    "        # reduced_data, cluster_labels, soft_labels, max_probs, index_info_df = apply_clustering(\n",
    "        #     latent_space, hdbscan_cfg, min_cluster_size, info_cfg[\"save_dir\"])\n",
    "        print(\">>> Clustering completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\">>> HDBSCAN clustering failed: {e}\")\n",
    "\n",
    "    # GPU Memory Cleanup\n",
    "    cp.get_default_memory_pool().free_all_blocks()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
